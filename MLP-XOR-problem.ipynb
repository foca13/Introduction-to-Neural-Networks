{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14e9e33-da43-427f-b577-1f2177293c4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3> MultiLayer perceptron (MLP)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67112b-310d-4e5b-9cef-7dae025a70ae",
   "metadata": {},
   "source": [
    "In this practical we will implement a 2 layer perceptron following the tutorial from [James Loy, Author of Neural Network Projects with Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6) to solve the XOR problem. You can find the github repository with the code [here](https://github.com/robmarkcole/Useful-python/blob/master/Numpy/Build%20a%20neural%20network.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc034a5-3204-4672-ab7b-607f1a946e48",
   "metadata": {},
   "source": [
    "We will create a class to represent our model and we will store the weights and biases as attributes. Additionally, our class will have methods to compute the forward and back propagation.\n",
    "\n",
    "To start, we need a couple of helper functions such as the sigmoid function and the loss function that we will define outside of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b547b0d-9c9c-4b95-a74e-265d4b7b5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91fdbf-5728-4814-b253-1fa4736ce445",
   "metadata": {},
   "source": [
    "**Exercise 1** - Define a function called `sigmoid` where you compute the sigmoid function $\\sigma(x)=\\frac{1}{1+e^{-x}}$ and another function called `mean_squared_error` where you compute the mean squared error: $MSE=\\frac{1}{n}\\sum_{i=0}^n(y_i-\\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66e8fa-2eb9-41f4-8dbb-f50b77d562be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d38fe9-56b3-4daf-b014-332bc98b23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise 2** - Create a class called NeuralNetwork. Your `init` should have no arguments but on it you should define 6 attributes: `weights_1`, a 2x2 matrix representing the connections from a 2 neuron input layer to a 2 neuron hidden layer; `bias_1`, a 2x1 vector representing the bias of the two neurons in the hidden layer; `weights_2`, a 2x1 matrix that represents the two connections from the hidden layer to the output; and `bias_2`, a 1x1 vector that represents the bias of the single neuron output layer. The last two attributes, `loss_per_epoch` and `epochs` will be used to monitor the training process of the neural network.\n",
    "\n",
    "**Exercise 3** - Fill the `forward`, `compute_gradients` & `train` methods. The forward computation is defined by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\sigma(W_2^T(\\sigma(W_1^TX+b_1))+b_2),\n",
    "\\end{equation}\n",
    "\n",
    "where $W_1$ is the previously defined attribute `weights_1`, $b_1$ is `bias_1`, $W_2$ is `weights_2`, $b_2$ is `bias_2` and $\\sigma$ is the sigmoid function.\n",
    "\n",
    "The `compute_gradients` method should return 4 values, which are the partial derivatives of the loss function with respect to the parameters of the model represented by the attribute matrices. Compute the gradient $\\nabla L$ in four steps:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W_2} = -2[(y - \\hat{y})\\hat{y}(1 - \\hat{y})]X_0^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b_2} = -2(y - \\hat{y})·\\hat{y}(1 - \\hat{y})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W_1} = -2(y - \\hat{y})·\\hat{y}(1 - \\hat{y})·W_2X^T\\odot(X_0(1-X_0))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b_1} = -2(y - \\hat{y})·\\hat{y}(1 - \\hat{y})·W_2\\odot(X_0(1-X_0))\n",
    "\\end{equation}\n",
    "\n",
    "where $X_0=\\sigma(W_1^TX+b_1)$ is the output of the first layer and $\\odot$ is the Hadamard (element-wise) product.\n",
    "\n",
    "Finally, the `train` method should use the input data to calculate the gradients and update the parameters of the model. In addition to `X` and `y`, it has two more arguments: the number of epochs and the learning rate. You should add the number of epochs to the value stored in the attribute `epochs`, and at each training step you shoud add the current loss to the attribute `loss_per_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec7171-cbe3-40c5-a693-0a3f38e3899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.weights_1 = None # fill this line\n",
    "        self.bias_1 = None # fill this line\n",
    "        self.weights_2 = None # fill this line\n",
    "        self.bias_2 = None # fill this line\n",
    "        self.loss_per_epoch = []\n",
    "        self.epochs = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        # your code goes here\n",
    "        pass\n",
    "\n",
    "    def compute_gradients(self, X, y):\n",
    "        # your code goes here\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate = 0.01):\n",
    "        # your code goes here\n",
    "        pass\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss_per_epoch)\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Loss (MSE)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588df3c-d22e-492e-9d4c-1dd964d07863",
   "metadata": {},
   "source": [
    "We are just missing the training data, which in this case is simply given by the XOR table:\n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "| :-------: | :-------: | :------: |\n",
    "|    0    |    0    |    0   |\n",
    "|    0    |    1    |    1   |\n",
    "|    1    |    0    |    1   |\n",
    "|    1    |    1    |    0   |\n",
    "\n",
    "**Exercise 4** - Train the model for 2500 epochs and learning rate 1 with the given data and plot the loss per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f273fd-dddb-483e-a0f1-8e19c9314f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).T\n",
    "y = np.array([0, 1, 1, 0])\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dabb84-7fc7-4b51-8fd9-fb137fa72d44",
   "metadata": {},
   "source": [
    "**Exercise 5** - Print the final raw predictions (without rounding) and compare them to the values on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c8ceb-9cac-410a-8494-5340af2a5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
